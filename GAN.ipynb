{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "m6tEPHcBIl0I"
      },
      "outputs": [],
      "source": [
        "# libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print ('Your system: ' + str(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrfS5WNJI1Ht",
        "outputId": "9da2a0c2-9b18-4fc1-af56-49babd904e53"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your system: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size_train = 128\n",
        "batch_size_test = 100\n",
        "drop_last = False\n",
        "num_workers = 2"
      ],
      "metadata": {
        "id": "qYdoRXcBhaXa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ MNIST ------------ #\n",
        "meanI = torch.tensor([0.5])\n",
        "stdI = torch.tensor([0.5])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Pad(2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = meanI, std = stdI),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Pad(2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = meanI, std = stdI),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=num_workers, drop_last=drop_last)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=False, num_workers=num_workers, drop_last=drop_last)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ATybfvSI41g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show\n",
        "# visualisation function\n",
        "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1, scale=2):\n",
        "    # Adapted from https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "    n,c,w,h = tensor.shape\n",
        "\n",
        "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
        "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
        "\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))\n",
        "    grid = torchvision.utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "    plt.figure( figsize=(nrow*scale,rows*scale) )\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.ioff()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XlbKLKzcJC5m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "img, _ = next(iter(trainloader))\n",
        "visTensor(img[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "-B8ClGPYhzLp",
        "outputId": "e5cc1749-ea57-47d3-d244-682af103a94f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x200 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwMAAACuCAYAAABqSNGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAQD0lEQVR4nO3dWaiV5dcA8H1SG/CiUSvM0oygySjKKVMLC1MigsokKhoQwQK1gSCa6CKotLyozGbrT2lKhGBFkRqllqGWWYHaXFBqVmYUZed/8fHxfc9Q+3WffQbP8/vdrcV6934657jPWbzvarW0tra21gAAgOLs1dkHAAAAOodmAAAACqUZAACAQmkGAACgUJoBAAAolGYAAAAKpRkAAIBCaQYAAKBQmgEAAChUz6qFLS0t7XkOAACgiVpbW+vWuDMAAACF0gwAAEChNAMAAFAozQAAABRKMwAAAIXSDAAAQKE0AwAAUCjNAAAAFEozAAAAhdIMAABAoTQDAABQKM0AAAAUSjMAAACF0gwAAEChNAMAAFAozQAAABRKMwAAAIXSDAAAQKE0AwAAUCjNAAAAFEozAAAAhdIMAABAoTQDAABQKM0AAAAUSjMAAACF0gwAAEChenb2AQBojmHDhiW5JUuWBPGBBx6Y1FxzzTVJ7sknn2zewQDostwZAACAQmkGAACgUJoBAAAolGYAAAAKZYAYoJu4/PLLk9wBBxwQxK2trUnN1KlTk5wBYmBP8NNPPwXxb7/9ltScc845Qbxhw4b2PNIex50BAAAolGYAAAAKpRkAAIBCmRkA6CaOPvrohq6bM2dOk09CV9SzZ/or/7rrrgvigQMHttv7v/XWW0nulVdeCeLc897wb+I5qMMOOyypmT59ehBfe+217XqmPY07AwAAUCjNAAAAFEozAAAAhdIMAABAoQwQd7KZM2cmuWnTpgXxXnulPdvff/8dxJMmTUpq3nnnnSD+9ttvGzgh0FVdcsklQXzuuefWveall15Kco899ljTzkTX0bdv3yCeO3duUnP++ed31HFq119/fZJbtWpVEK9YsSKpmT17dhB//fXXzT0YFM6dAQAAKJRmAAAACqUZAACAQpkZaEfz589PcvFyjIsvvjipiecBcuKa559/PqlZuXJlEI8cObLu69I9jBkz5l/jnDvuuCPJLVu2LIjPOuusNpyKZhs8eHAQt7S01L3GUqdyxN/rnTt3JjXr1q0L4s2bNyc169evD+J4FqFWq9VGjBhR9zy5n89BgwYF8dChQ5Oaq666KognT56c1KxduzaIP/vss7rnAf6HOwMAAFAozQAAABRKMwAAAIXSDAAAQKFaWuOJ1n8qrDCYVpJ48DdeFFar1WrDhw9PcvGXO7cILF66cuSRRyY18ZBV7vsTv1ePHj2SGrq23OBvnMsN/raXeKC4VjNU3JmWLFkSxOPGjat7zQUXXJDkFi9e3LQz0XX17t07yZ1yyilBvH379qRmw4YN7Xam4447LohnzJiR1Fx99dV1X2fr1q1B/NxzzyU1t956axD//vvvVY5IF9K/f/8kF//PWoYNG5bUxMvtRo0aldT89ddfbTxd11Tlz3x3BgAAoFCaAQAAKJRmAAAACmVmoILcM4z33XdfEOcWhe21V9prxXW5RWDvvvtuEPfr1y+piZ+Ry80nxO/Vq1evpIbOc+eddwZxRz7730wlfzZ0pBNOOCHJxQujcnNB27ZtC+JDDz00qamy6JA938yZM5NcPO+WmxmIn8HetGlTU8/1/+27775JbsKECUH88MMPJzWHHHJI3dceP358EL/22mu7eTo6W+5vnaVLlwbx3nvvndTMnTs3iKdMmdLcg3VhZgYAAIB/pBkAAIBCaQYAAKBQmgEAAChUz84+QEc74ogjgjg3jLJgwYIgzg3XxUOTuWHhhQsXJrl4kCMeFs4ZMWJEkovPnRvizJ2pFFWWdeWMHj267jV33XVX3dfp7GHgeDnY8uXLk5p4gLni/0uAThIvOqzVqi0S/Oijj4K46rBw/Jly0EEH1b3mp59+SnK7du2q9H40X7yg6aqrrqp7zccff5zkvvnmm6adqZ7cIrBFixYF8Zo1a5Kau+++O4gnTZqU1MSLyG6++eakZt68eUHs57fzDBw4MMm9/PLLSS43MBx75JFHmnKm7qrcvxYBAKBwmgEAACiUZgAAAApV3MxAvKxryJAhSU38TG3uGdv4Wf8HHnggqYmfc2xUvBQmd6YqC85KEi8haaaOnAeoMp8QP/tfVZUZilg8i0DHufTSSxu6bvXq1XVr9t9//yQ3ffr0IL799tvrvk7u390VV1wRxN9++23d16E54t9vue9zLF6oWavln+PvTJ9//nmSmzx5chDnfidOnDgxiB9//PGk5uCDDw7i+++/v5Ej0gR9+vRJclWWy23dujXJ5eaZ+D/uDAAAQKE0AwAAUCjNAAAAFEozAAAAhSpugDhe4JUbsq2yrGv27NlBvGrVqobOEy9Bq9XShWK5xWjxgqjc0rEqC826q9ygayMDs42qMvgbn7Gjh3MbGbI+66yz2uEktKf33nuvbs3UqVOTXJWB4Vju5+PNN98M4tNOOy2p2bFjx26/F6FRo0YluRtvvLHudQ899FAQv/rqq007U0f67bffgvjee+9NasaNGxfEuYHqsWPHBrEB4s6zzz77NHTds88+m+S+/PLLth6nW3NnAAAACqUZAACAQmkGAACgUC2t8cPn/1SYeSZ9T7Rr164grjIzUKVm5cqVSU1uuU785e7fv39SEy+KqbJQLDcfEC9YsewnlFvWFS8Uyz3739nP+jdLxX/6ge7yObAn+vTTT5PcscceG8SffPJJUnP66acHcW4GKfeceI8ePYL4zz//TGqeeOKJIL7wwguTmkMPPTSIp0yZktTMnTs3yfHvevXqFcS5z6Fhw4YF8ffff5/UjB49Oog3btzY9sN1UU8//XQQX3755XWvif8d0HFyz/5fdtllda+bNGlSkosXzpakyu96dwYAAKBQmgEAACiUZgAAAAqlGQAAgEIVt3QsHgaaMWNGUhMvGckN8MaDlLmhvNywZZVlYVVq4jONHDkyqeHf5QaIc7nuoNH/rj11OLpUxx13XJKbMGFCEOc+83JDkvHA8BVXXJHUxEN5uYVi8QDxQQcdlNSw+5YvXx7E8bBwrZb+Lsktl+vOA8Oxa6+9NojjJWS1Wq3Wt2/fjjoOkXgofuDAgZ10kvK4MwAAAIXSDAAAQKE0AwAAUCjNAAAAFKq4AeLYrFmzktxXX30VxLntbfGw1rRp05KaKpuDG62ZOXNmkoP/NWbMmCCONytXldvAzJ7ltttuC+ITTzyx0nXxgGqzNnjmhozZffHvoNzvqXXr1gXxSy+91J5H6vLmzZsXxH369Elqfv311446DpGxY8cG8YgRIypdt3PnziD+8MMPm3amUrgzAAAAhdIMAABAoTQDAABQqOJnBnIWLlxYt2bRokVBfNNNNyU1K1asSHJDhw4N4ioLxXILgeDfxDMDVeTmAywd6zoWL16c5G644Ya611WZEdiyZUuSy32mxeLPr9x8UyyeyaI5fv755yQ3ceLETjhJ5xg1alQQX3nllUnN+PHj677Oueee27QzsXsGDBjQ0HWbNm0K4k8++aQJpymLOwMAAFAozQAAABRKMwAAAIXSDAAAQKEMELej3BKYRpaOwe5qZMmYYeGurT0X6fznP/9Jch988EHd6+Jhy1NPPbXuNYMHD65+MCr766+/ktyPP/7YCSdpm7333jvJHX/88UGcG4y+5pprgvjggw+u+15PP/10klu9enXd62gfjS65vOiii5p8kvK4MwAAAIXSDAAAQKE0AwAAUCgzA00ybNiwJDd8+PAkF88R5BaclbQohrZbunRpQ9fFMwJmBrq2V155Jcnt3LkziHv37t3Qa1dZtJh77SeffHK33yte2Ehz5J6Rnzp1ahDffffdHXWcrKOOOirJxc/6x8vDarVa7cwzz6z72tu3bw/it99+O6mZM2dOEOd+Fnft2lX3vWi7WbNmJblDDjmk7nVr165Nct98801TzlQydwYAAKBQmgEAACiUZgAAAAqlGQAAgEIZIG7Q/Pnzg3jo0KFJTZWlY7khGtgdY8aMaei6Rhe80Dm2bt2a5N56660gPu+88xp67Q0bNiS5Xr16BXFuyPjwww+v+9rPPPNMED/++OO7eTpyPvvssyAeOHBgUnPLLbcE8TnnnFP3dTdv3pzkFixYEMRTpkxJag488MC6rz1gwIAk169fv7rXxd5///0kd/vttwfxa6+9ttuvS8cZMmRI3ZqWlpYkl/u+/vHHH005U8ncGQAAgEJpBgAAoFCaAQAAKFRLa+7B9lxh5tmtUuQWiq1YsSKIc1/G3Ncsvm7kyJFtPB2lq/hPOFHyv+nuIp4Xef3115OaHj161H2d3DPY8TPpl1xySd3X+eKLL5JcfMavvvqq7utQ35FHHhnEue/9Mccc01HHqST3mbNmzZogXr16dVITLwuLfzZrtVptx44dbTwd7Sn+/HjqqaeSmv322y+I169fn9Tk5jN///33Np6ue6vyN4I7AwAAUCjNAAAAFEozAAAAhdIMAABAoSwdy2hkoVi8TKxWq9XefffdJDdx4sQ2no7SNbJkzIKx7mnZsmVBPGnSpKTmnnvuCeJBgwYlNaeddlqlXGzbtm1139/AcPuIv65nnHFGUnPZZZcF8UknnZTUnHzyyUH8wQcfNOF0tdr999+f5P78888kt2XLliD+5ZdfmvL+dJ6ePdM/LadPnx7E8bBwzsyZM5OcYeH24c4AAAAUSjMAAACF0gwAAEChipsZOOKII4I4ng+o1Wq14cOHB3GVhWK5+QALxWgPjcwMUIaFCxcmubVr1wbxo48+mtScffbZdV/7zTffTHKTJ08O4twyKDrG1q1bk9zs2bPrXte7d+8g3rlzZ9PORJkuvPDCJNevX7+6173wwgtB/MYbbzTtTPw7dwYAAKBQmgEAACiUZgAAAAqlGQAAgEJ1qwHieBg4N/jbv3//IB4yZEhSU2WhWLxcZ+XKlZXPCW0xevTozj4Ce5DNmzcH8dixYzvpJHRFBoZptuXLlye5HTt2BPHGjRuTmvXr1wfxd99919yD8Y/cGQAAgEJpBgAAoFCaAQAAKNQeOzOQWxZ28cUXB3GVZWG5mnhxT5UaaA933nlnkrN0DICu6ocffkhyJ5xwQiechKrcGQAAgEJpBgAAoFCaAQAAKJRmAAAACrXHDhDPmjUryb344otBPG3atKTmwQcfDOLccPCiRYvadDboapYtW9bZRwAAuiB3BgAAoFCaAQAAKJRmAAAACtXSmntoPlcYLesC2l9uwdjSpUvrXnfXXXcFcW55GQDQvVX5M9+dAQAAKJRmAAAACqUZAACAQmkGAACgUAaIAQCgGzJADAAA/CPNAAAAFEozAAAAhdIMAABAoTQDAABQKM0AAAAUSjMAAACF0gwAAEChNAMAAFAozQAAABRKMwAAAIXSDAAAQKE0AwAAUCjNAAAAFEozAAAAhdIMAABAoTQDAABQqJ5VC1tbW9vzHAAAQAdzZwAAAAqlGQAAgEJpBgAAoFCaAQAAKJRmAAAACqUZAACAQmkGAACgUJoBAAAolGYAAAAK9V97O/zG/XPH+QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 1\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 20\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "metadata": {
        "id": "RMcseuZ9fMtB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*8) x 4 x 4``\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*4) x 8 x 8``\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # # state size. ``(ngf*2) x 16 x 16``\n",
        "            # nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            # nn.BatchNorm2d(ngf),\n",
        "            # nn.ReLU(True),\n",
        "            # state size. ``(ngf) x 32 x 32``\n",
        "            nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. ``(nc) x 64 x 64``\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "yIJ-eBtEe-qW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is ``(nc) x 64 x 64``\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf) x 32 x 32``\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*2) x 16 x 16``\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # # state size. ``(ndf*4) x 8 x 8``\n",
        "            # nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            # nn.BatchNorm2d(ndf * 8),\n",
        "            # nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*8) x 4 x 4``\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "TiXQArHEfgMr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "netG = Generator().to(device)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnDmIMgVlUwe",
        "outputId": "a75c8a0a-b0bb-4e59-d1f3-5df11c3ceb7d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr2O0Ig-gz5X",
        "outputId": "9ab53099-024a-4576-a008-661f5294e922"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ``BCELoss`` function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "aMFBKUjSg2yj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(trainloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        # if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "        #     with torch.no_grad():\n",
        "        #         fake = netG(fixed_noise).detach().cpu()\n",
        "        #     img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBkF8t67g7Ll",
        "outputId": "ac254932-b9c7-42bd-efb7-0f445dad1ef2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "[0/20][0/469]\tLoss_D: 1.4997\tLoss_G: 0.9985\tD(x): 0.4012\tD(G(z)): 0.4294 / 0.3755\n",
            "[0/20][50/469]\tLoss_D: 0.0261\tLoss_G: 5.4082\tD(x): 0.9916\tD(G(z)): 0.0174 / 0.0046\n",
            "[0/20][100/469]\tLoss_D: 0.0136\tLoss_G: 5.8746\tD(x): 0.9942\tD(G(z)): 0.0077 / 0.0028\n",
            "[0/20][150/469]\tLoss_D: 0.0259\tLoss_G: 5.8392\tD(x): 0.9794\tD(G(z)): 0.0048 / 0.0031\n",
            "[0/20][200/469]\tLoss_D: 0.6644\tLoss_G: 4.2687\tD(x): 0.6033\tD(G(z)): 0.0154 / 0.0213\n",
            "[0/20][250/469]\tLoss_D: 0.4144\tLoss_G: 3.0101\tD(x): 0.8255\tD(G(z)): 0.1749 / 0.0578\n",
            "[0/20][300/469]\tLoss_D: 1.3295\tLoss_G: 2.7324\tD(x): 0.7297\tD(G(z)): 0.5725 / 0.0896\n",
            "[0/20][350/469]\tLoss_D: 0.8234\tLoss_G: 1.5174\tD(x): 0.5876\tD(G(z)): 0.2004 / 0.2393\n",
            "[0/20][400/469]\tLoss_D: 0.5989\tLoss_G: 1.9274\tD(x): 0.6928\tD(G(z)): 0.1764 / 0.1667\n",
            "[0/20][450/469]\tLoss_D: 0.8780\tLoss_G: 1.2070\tD(x): 0.5774\tD(G(z)): 0.2224 / 0.3404\n",
            "[1/20][0/469]\tLoss_D: 0.9008\tLoss_G: 1.5008\tD(x): 0.6445\tD(G(z)): 0.3395 / 0.2528\n",
            "[1/20][50/469]\tLoss_D: 0.9874\tLoss_G: 0.6715\tD(x): 0.4905\tD(G(z)): 0.1996 / 0.5262\n",
            "[1/20][100/469]\tLoss_D: 1.2196\tLoss_G: 0.9255\tD(x): 0.3945\tD(G(z)): 0.1786 / 0.4195\n",
            "[1/20][150/469]\tLoss_D: 0.9544\tLoss_G: 1.5497\tD(x): 0.7537\tD(G(z)): 0.4674 / 0.2313\n",
            "[1/20][200/469]\tLoss_D: 1.0937\tLoss_G: 2.3055\tD(x): 0.8613\tD(G(z)): 0.5945 / 0.1101\n",
            "[1/20][250/469]\tLoss_D: 0.8582\tLoss_G: 1.2223\tD(x): 0.6548\tD(G(z)): 0.3311 / 0.3123\n",
            "[1/20][300/469]\tLoss_D: 0.8014\tLoss_G: 0.7899\tD(x): 0.5601\tD(G(z)): 0.1701 / 0.4736\n",
            "[1/20][350/469]\tLoss_D: 0.7960\tLoss_G: 1.4318\tD(x): 0.7800\tD(G(z)): 0.4037 / 0.2546\n",
            "[1/20][400/469]\tLoss_D: 1.0325\tLoss_G: 2.2252\tD(x): 0.8649\tD(G(z)): 0.5681 / 0.1223\n",
            "[1/20][450/469]\tLoss_D: 0.9724\tLoss_G: 0.8579\tD(x): 0.4381\tD(G(z)): 0.0942 / 0.4466\n",
            "[2/20][0/469]\tLoss_D: 1.4926\tLoss_G: 0.1569\tD(x): 0.2614\tD(G(z)): 0.0531 / 0.8571\n",
            "[2/20][50/469]\tLoss_D: 1.0940\tLoss_G: 1.1259\tD(x): 0.4064\tD(G(z)): 0.1170 / 0.3486\n",
            "[2/20][100/469]\tLoss_D: 0.9451\tLoss_G: 1.9446\tD(x): 0.8793\tD(G(z)): 0.5392 / 0.1593\n",
            "[2/20][150/469]\tLoss_D: 1.1829\tLoss_G: 2.7282\tD(x): 0.9008\tD(G(z)): 0.6383 / 0.0749\n",
            "[2/20][200/469]\tLoss_D: 0.7511\tLoss_G: 1.1423\tD(x): 0.6333\tD(G(z)): 0.2287 / 0.3408\n",
            "[2/20][250/469]\tLoss_D: 0.7260\tLoss_G: 2.2835\tD(x): 0.8146\tD(G(z)): 0.3895 / 0.1119\n",
            "[2/20][300/469]\tLoss_D: 0.7390\tLoss_G: 1.6831\tD(x): 0.7802\tD(G(z)): 0.3666 / 0.2025\n",
            "[2/20][350/469]\tLoss_D: 0.7568\tLoss_G: 1.1881\tD(x): 0.7208\tD(G(z)): 0.3297 / 0.3240\n",
            "[2/20][400/469]\tLoss_D: 0.6690\tLoss_G: 2.4184\tD(x): 0.8168\tD(G(z)): 0.3530 / 0.1030\n",
            "[2/20][450/469]\tLoss_D: 0.6642\tLoss_G: 1.6159\tD(x): 0.7018\tD(G(z)): 0.2433 / 0.2183\n",
            "[3/20][0/469]\tLoss_D: 0.6688\tLoss_G: 1.2076\tD(x): 0.6670\tD(G(z)): 0.2028 / 0.3217\n",
            "[3/20][50/469]\tLoss_D: 0.6537\tLoss_G: 1.3342\tD(x): 0.6906\tD(G(z)): 0.2169 / 0.2948\n",
            "[3/20][100/469]\tLoss_D: 1.0117\tLoss_G: 1.2212\tD(x): 0.7567\tD(G(z)): 0.4861 / 0.3232\n",
            "[3/20][150/469]\tLoss_D: 0.7050\tLoss_G: 1.5881\tD(x): 0.6973\tD(G(z)): 0.2586 / 0.2292\n",
            "[3/20][200/469]\tLoss_D: 0.6870\tLoss_G: 1.4926\tD(x): 0.6907\tD(G(z)): 0.2371 / 0.2617\n",
            "[3/20][250/469]\tLoss_D: 0.6933\tLoss_G: 1.1242\tD(x): 0.6638\tD(G(z)): 0.2167 / 0.3507\n",
            "[3/20][300/469]\tLoss_D: 0.6332\tLoss_G: 3.0050\tD(x): 0.8364\tD(G(z)): 0.3368 / 0.0627\n",
            "[3/20][350/469]\tLoss_D: 0.6417\tLoss_G: 1.5107\tD(x): 0.6834\tD(G(z)): 0.2051 / 0.2446\n",
            "[3/20][400/469]\tLoss_D: 0.9361\tLoss_G: 0.4050\tD(x): 0.4660\tD(G(z)): 0.0968 / 0.6851\n",
            "[3/20][450/469]\tLoss_D: 0.7215\tLoss_G: 1.6933\tD(x): 0.8443\tD(G(z)): 0.3910 / 0.2135\n",
            "[4/20][0/469]\tLoss_D: 1.3246\tLoss_G: 0.6065\tD(x): 0.3322\tD(G(z)): 0.0838 / 0.5747\n",
            "[4/20][50/469]\tLoss_D: 0.7090\tLoss_G: 1.8402\tD(x): 0.7123\tD(G(z)): 0.2820 / 0.1792\n",
            "[4/20][100/469]\tLoss_D: 0.6221\tLoss_G: 2.1283\tD(x): 0.7893\tD(G(z)): 0.2970 / 0.1358\n",
            "[4/20][150/469]\tLoss_D: 0.6973\tLoss_G: 1.2208\tD(x): 0.6432\tD(G(z)): 0.1944 / 0.3301\n",
            "[4/20][200/469]\tLoss_D: 0.7009\tLoss_G: 2.7520\tD(x): 0.8201\tD(G(z)): 0.3631 / 0.0778\n",
            "[4/20][250/469]\tLoss_D: 0.6865\tLoss_G: 1.3013\tD(x): 0.6205\tD(G(z)): 0.1435 / 0.3108\n",
            "[4/20][300/469]\tLoss_D: 0.6333\tLoss_G: 1.2500\tD(x): 0.7591\tD(G(z)): 0.2773 / 0.3124\n",
            "[4/20][350/469]\tLoss_D: 0.6861\tLoss_G: 2.4994\tD(x): 0.8720\tD(G(z)): 0.3941 / 0.0986\n",
            "[4/20][400/469]\tLoss_D: 0.5554\tLoss_G: 1.3852\tD(x): 0.7384\tD(G(z)): 0.1946 / 0.2773\n",
            "[4/20][450/469]\tLoss_D: 0.5630\tLoss_G: 1.8612\tD(x): 0.7744\tD(G(z)): 0.2380 / 0.1868\n",
            "[5/20][0/469]\tLoss_D: 0.9406\tLoss_G: 0.7135\tD(x): 0.4674\tD(G(z)): 0.0910 / 0.5219\n",
            "[5/20][50/469]\tLoss_D: 0.5744\tLoss_G: 1.4140\tD(x): 0.7280\tD(G(z)): 0.2034 / 0.2787\n",
            "[5/20][100/469]\tLoss_D: 1.2331\tLoss_G: 0.8451\tD(x): 0.4953\tD(G(z)): 0.3387 / 0.4577\n",
            "[5/20][150/469]\tLoss_D: 0.6964\tLoss_G: 1.8496\tD(x): 0.7468\tD(G(z)): 0.2998 / 0.1860\n",
            "[5/20][200/469]\tLoss_D: 0.7113\tLoss_G: 0.7336\tD(x): 0.6024\tD(G(z)): 0.1393 / 0.5202\n",
            "[5/20][250/469]\tLoss_D: 1.2166\tLoss_G: 1.9742\tD(x): 0.9153\tD(G(z)): 0.6202 / 0.1678\n",
            "[5/20][300/469]\tLoss_D: 0.7152\tLoss_G: 2.0126\tD(x): 0.8170\tD(G(z)): 0.3703 / 0.1610\n",
            "[5/20][350/469]\tLoss_D: 0.9057\tLoss_G: 2.6631\tD(x): 0.8516\tD(G(z)): 0.4872 / 0.0864\n",
            "[5/20][400/469]\tLoss_D: 0.6457\tLoss_G: 2.3831\tD(x): 0.8478\tD(G(z)): 0.3534 / 0.1105\n",
            "[5/20][450/469]\tLoss_D: 0.7179\tLoss_G: 1.7930\tD(x): 0.6049\tD(G(z)): 0.1409 / 0.1984\n",
            "[6/20][0/469]\tLoss_D: 1.0572\tLoss_G: 1.3960\tD(x): 0.4342\tD(G(z)): 0.0893 / 0.2971\n",
            "[6/20][50/469]\tLoss_D: 0.5684\tLoss_G: 2.0430\tD(x): 0.8090\tD(G(z)): 0.2770 / 0.1546\n",
            "[6/20][100/469]\tLoss_D: 0.5839\tLoss_G: 1.9143\tD(x): 0.7638\tD(G(z)): 0.2420 / 0.1784\n",
            "[6/20][150/469]\tLoss_D: 0.7272\tLoss_G: 1.4971\tD(x): 0.7470\tD(G(z)): 0.3182 / 0.2526\n",
            "[6/20][200/469]\tLoss_D: 0.6674\tLoss_G: 1.8835\tD(x): 0.6875\tD(G(z)): 0.2110 / 0.1809\n",
            "[6/20][250/469]\tLoss_D: 0.5351\tLoss_G: 1.6820\tD(x): 0.7524\tD(G(z)): 0.1859 / 0.2229\n",
            "[6/20][300/469]\tLoss_D: 0.6456\tLoss_G: 2.0260\tD(x): 0.7465\tD(G(z)): 0.2596 / 0.1559\n",
            "[6/20][350/469]\tLoss_D: 0.5908\tLoss_G: 1.4809\tD(x): 0.7198\tD(G(z)): 0.1943 / 0.2758\n",
            "[6/20][400/469]\tLoss_D: 0.9113\tLoss_G: 3.0556\tD(x): 0.9204\tD(G(z)): 0.5088 / 0.0651\n",
            "[6/20][450/469]\tLoss_D: 0.5234\tLoss_G: 2.5498\tD(x): 0.8269\tD(G(z)): 0.2569 / 0.0980\n",
            "[7/20][0/469]\tLoss_D: 0.8200\tLoss_G: 2.2905\tD(x): 0.7525\tD(G(z)): 0.3718 / 0.1276\n",
            "[7/20][50/469]\tLoss_D: 0.5959\tLoss_G: 2.6776\tD(x): 0.8775\tD(G(z)): 0.3372 / 0.0919\n",
            "[7/20][100/469]\tLoss_D: 0.5086\tLoss_G: 2.1788\tD(x): 0.7899\tD(G(z)): 0.2114 / 0.1363\n",
            "[7/20][150/469]\tLoss_D: 0.6399\tLoss_G: 1.6655\tD(x): 0.8301\tD(G(z)): 0.3168 / 0.2310\n",
            "[7/20][200/469]\tLoss_D: 0.7859\tLoss_G: 2.3650\tD(x): 0.7974\tD(G(z)): 0.3852 / 0.1200\n",
            "[7/20][250/469]\tLoss_D: 1.0140\tLoss_G: 2.1393\tD(x): 0.8740\tD(G(z)): 0.5370 / 0.1507\n",
            "[7/20][300/469]\tLoss_D: 0.8749\tLoss_G: 0.6396\tD(x): 0.5224\tD(G(z)): 0.1137 / 0.5617\n",
            "[7/20][350/469]\tLoss_D: 0.7275\tLoss_G: 2.1640\tD(x): 0.8895\tD(G(z)): 0.4213 / 0.1397\n",
            "[7/20][400/469]\tLoss_D: 0.7430\tLoss_G: 1.5101\tD(x): 0.6776\tD(G(z)): 0.2464 / 0.2633\n",
            "[7/20][450/469]\tLoss_D: 0.5637\tLoss_G: 2.4710\tD(x): 0.7738\tD(G(z)): 0.2372 / 0.1052\n",
            "[8/20][0/469]\tLoss_D: 0.5831\tLoss_G: 1.7691\tD(x): 0.7617\tD(G(z)): 0.2358 / 0.2101\n",
            "[8/20][50/469]\tLoss_D: 0.6628\tLoss_G: 1.9315\tD(x): 0.8864\tD(G(z)): 0.3923 / 0.1694\n",
            "[8/20][100/469]\tLoss_D: 1.1417\tLoss_G: 0.6485\tD(x): 0.4240\tD(G(z)): 0.1294 / 0.5573\n",
            "[8/20][150/469]\tLoss_D: 0.6300\tLoss_G: 1.5056\tD(x): 0.6772\tD(G(z)): 0.1645 / 0.2609\n",
            "[8/20][200/469]\tLoss_D: 0.4871\tLoss_G: 2.4466\tD(x): 0.7701\tD(G(z)): 0.1647 / 0.1120\n",
            "[8/20][250/469]\tLoss_D: 0.4931\tLoss_G: 2.5783\tD(x): 0.8383\tD(G(z)): 0.2451 / 0.0956\n",
            "[8/20][300/469]\tLoss_D: 0.5104\tLoss_G: 2.1772\tD(x): 0.7888\tD(G(z)): 0.2100 / 0.1424\n",
            "[8/20][350/469]\tLoss_D: 0.5760\tLoss_G: 1.5479\tD(x): 0.6614\tD(G(z)): 0.1106 / 0.2523\n",
            "[8/20][400/469]\tLoss_D: 0.6170\tLoss_G: 1.8964\tD(x): 0.6617\tD(G(z)): 0.1458 / 0.1846\n",
            "[8/20][450/469]\tLoss_D: 0.6106\tLoss_G: 2.0905\tD(x): 0.8297\tD(G(z)): 0.3109 / 0.1539\n",
            "[9/20][0/469]\tLoss_D: 0.6824\tLoss_G: 1.5399\tD(x): 0.7279\tD(G(z)): 0.2629 / 0.2630\n",
            "[9/20][50/469]\tLoss_D: 0.6990\tLoss_G: 2.0023\tD(x): 0.7736\tD(G(z)): 0.3198 / 0.1698\n",
            "[9/20][100/469]\tLoss_D: 0.5056\tLoss_G: 1.9658\tD(x): 0.7558\tD(G(z)): 0.1792 / 0.1776\n",
            "[9/20][150/469]\tLoss_D: 0.5756\tLoss_G: 1.4880\tD(x): 0.6721\tD(G(z)): 0.1278 / 0.2735\n",
            "[9/20][200/469]\tLoss_D: 0.5871\tLoss_G: 2.2270\tD(x): 0.8422\tD(G(z)): 0.2971 / 0.1389\n",
            "[9/20][250/469]\tLoss_D: 0.5521\tLoss_G: 2.0291\tD(x): 0.6784\tD(G(z)): 0.1090 / 0.1646\n",
            "[9/20][300/469]\tLoss_D: 0.7975\tLoss_G: 1.9163\tD(x): 0.5487\tD(G(z)): 0.0928 / 0.1918\n",
            "[9/20][350/469]\tLoss_D: 0.5381\tLoss_G: 2.4726\tD(x): 0.8231\tD(G(z)): 0.2606 / 0.1069\n",
            "[9/20][400/469]\tLoss_D: 0.7838\tLoss_G: 1.4724\tD(x): 0.5774\tD(G(z)): 0.1358 / 0.2746\n",
            "[9/20][450/469]\tLoss_D: 0.7198\tLoss_G: 1.0631\tD(x): 0.5984\tD(G(z)): 0.1339 / 0.3841\n",
            "[10/20][0/469]\tLoss_D: 0.4607\tLoss_G: 1.8697\tD(x): 0.8547\tD(G(z)): 0.2408 / 0.1840\n",
            "[10/20][50/469]\tLoss_D: 0.5271\tLoss_G: 2.3939\tD(x): 0.7967\tD(G(z)): 0.2350 / 0.1131\n",
            "[10/20][100/469]\tLoss_D: 0.8875\tLoss_G: 3.2319\tD(x): 0.9182\tD(G(z)): 0.4990 / 0.0581\n",
            "[10/20][150/469]\tLoss_D: 0.6351\tLoss_G: 1.9376\tD(x): 0.7343\tD(G(z)): 0.2395 / 0.1780\n",
            "[10/20][200/469]\tLoss_D: 0.7031\tLoss_G: 1.3564\tD(x): 0.7162\tD(G(z)): 0.2581 / 0.3078\n",
            "[10/20][250/469]\tLoss_D: 0.8720\tLoss_G: 0.8082\tD(x): 0.5827\tD(G(z)): 0.2074 / 0.4790\n",
            "[10/20][300/469]\tLoss_D: 0.4952\tLoss_G: 2.3087\tD(x): 0.7720\tD(G(z)): 0.1762 / 0.1263\n",
            "[10/20][350/469]\tLoss_D: 0.5930\tLoss_G: 2.0288\tD(x): 0.7499\tD(G(z)): 0.2215 / 0.1703\n",
            "[10/20][400/469]\tLoss_D: 0.8016\tLoss_G: 0.9530\tD(x): 0.5785\tD(G(z)): 0.1681 / 0.4240\n",
            "[10/20][450/469]\tLoss_D: 0.6536\tLoss_G: 2.2351\tD(x): 0.8548\tD(G(z)): 0.3435 / 0.1459\n",
            "[11/20][0/469]\tLoss_D: 1.0364\tLoss_G: 3.6021\tD(x): 0.9602\tD(G(z)): 0.5735 / 0.0376\n",
            "[11/20][50/469]\tLoss_D: 0.7216\tLoss_G: 2.7935\tD(x): 0.9390\tD(G(z)): 0.4433 / 0.0811\n",
            "[11/20][100/469]\tLoss_D: 0.8059\tLoss_G: 2.7897\tD(x): 0.8906\tD(G(z)): 0.4506 / 0.0804\n",
            "[11/20][150/469]\tLoss_D: 0.5374\tLoss_G: 2.4217\tD(x): 0.7909\tD(G(z)): 0.2326 / 0.1131\n",
            "[11/20][200/469]\tLoss_D: 0.7058\tLoss_G: 0.8473\tD(x): 0.5920\tD(G(z)): 0.1061 / 0.4779\n",
            "[11/20][250/469]\tLoss_D: 0.6622\tLoss_G: 1.2749\tD(x): 0.6660\tD(G(z)): 0.1778 / 0.3243\n",
            "[11/20][300/469]\tLoss_D: 0.4999\tLoss_G: 1.2366\tD(x): 0.7044\tD(G(z)): 0.1027 / 0.3386\n",
            "[11/20][350/469]\tLoss_D: 0.8918\tLoss_G: 1.0874\tD(x): 0.5514\tD(G(z)): 0.1867 / 0.3859\n",
            "[11/20][400/469]\tLoss_D: 0.7897\tLoss_G: 2.6159\tD(x): 0.9485\tD(G(z)): 0.4708 / 0.0997\n",
            "[11/20][450/469]\tLoss_D: 0.5850\tLoss_G: 1.2914\tD(x): 0.6592\tD(G(z)): 0.1048 / 0.3366\n",
            "[12/20][0/469]\tLoss_D: 0.7353\tLoss_G: 1.6425\tD(x): 0.6157\tD(G(z)): 0.1586 / 0.2392\n",
            "[12/20][50/469]\tLoss_D: 0.6820\tLoss_G: 1.7093\tD(x): 0.7448\tD(G(z)): 0.2732 / 0.2150\n",
            "[12/20][100/469]\tLoss_D: 0.7474\tLoss_G: 3.3789\tD(x): 0.8595\tD(G(z)): 0.4029 / 0.0474\n",
            "[12/20][150/469]\tLoss_D: 0.5889\tLoss_G: 1.5830\tD(x): 0.6945\tD(G(z)): 0.1552 / 0.2511\n",
            "[12/20][200/469]\tLoss_D: 0.7813\tLoss_G: 2.1397\tD(x): 0.6358\tD(G(z)): 0.2219 / 0.1592\n",
            "[12/20][250/469]\tLoss_D: 0.6638\tLoss_G: 2.2348\tD(x): 0.8851\tD(G(z)): 0.3841 / 0.1358\n",
            "[12/20][300/469]\tLoss_D: 0.4331\tLoss_G: 2.4343\tD(x): 0.8126\tD(G(z)): 0.1743 / 0.1160\n",
            "[12/20][350/469]\tLoss_D: 0.6002\tLoss_G: 1.4812\tD(x): 0.6333\tD(G(z)): 0.0821 / 0.2789\n",
            "[12/20][400/469]\tLoss_D: 0.4979\tLoss_G: 2.5034\tD(x): 0.8256\tD(G(z)): 0.2324 / 0.1165\n",
            "[12/20][450/469]\tLoss_D: 0.5732\tLoss_G: 2.4235\tD(x): 0.8470\tD(G(z)): 0.2965 / 0.1171\n",
            "[13/20][0/469]\tLoss_D: 0.6334\tLoss_G: 2.3962\tD(x): 0.9035\tD(G(z)): 0.3766 / 0.1167\n",
            "[13/20][50/469]\tLoss_D: 0.6437\tLoss_G: 3.2649\tD(x): 0.8967\tD(G(z)): 0.3673 / 0.0534\n",
            "[13/20][100/469]\tLoss_D: 0.6501\tLoss_G: 3.2388\tD(x): 0.8550\tD(G(z)): 0.3473 / 0.0546\n",
            "[13/20][150/469]\tLoss_D: 0.4019\tLoss_G: 2.5132\tD(x): 0.8354\tD(G(z)): 0.1739 / 0.1087\n",
            "[13/20][200/469]\tLoss_D: 0.5055\tLoss_G: 1.8755\tD(x): 0.7058\tD(G(z)): 0.1093 / 0.1929\n",
            "[13/20][250/469]\tLoss_D: 0.7211\tLoss_G: 2.2557\tD(x): 0.7954\tD(G(z)): 0.3430 / 0.1411\n",
            "[13/20][300/469]\tLoss_D: 0.7932\tLoss_G: 2.2663\tD(x): 0.8488\tD(G(z)): 0.4075 / 0.1324\n",
            "[13/20][350/469]\tLoss_D: 0.6945\tLoss_G: 1.5870\tD(x): 0.6860\tD(G(z)): 0.2167 / 0.2588\n",
            "[13/20][400/469]\tLoss_D: 0.5180\tLoss_G: 2.1656\tD(x): 0.8202\tD(G(z)): 0.2311 / 0.1495\n",
            "[13/20][450/469]\tLoss_D: 0.5662\tLoss_G: 2.0424\tD(x): 0.7423\tD(G(z)): 0.1951 / 0.1624\n",
            "[14/20][0/469]\tLoss_D: 0.4103\tLoss_G: 1.6579\tD(x): 0.7932\tD(G(z)): 0.1412 / 0.2302\n",
            "[14/20][50/469]\tLoss_D: 0.5989\tLoss_G: 2.7086\tD(x): 0.9235\tD(G(z)): 0.3624 / 0.0897\n",
            "[14/20][100/469]\tLoss_D: 0.4888\tLoss_G: 2.0237\tD(x): 0.7695\tD(G(z)): 0.1662 / 0.1643\n",
            "[14/20][150/469]\tLoss_D: 0.3972\tLoss_G: 2.5240\tD(x): 0.8141\tD(G(z)): 0.1483 / 0.1133\n",
            "[14/20][200/469]\tLoss_D: 0.6348\tLoss_G: 3.1497\tD(x): 0.8803\tD(G(z)): 0.3524 / 0.0606\n",
            "[14/20][250/469]\tLoss_D: 0.9857\tLoss_G: 1.2172\tD(x): 0.4973\tD(G(z)): 0.1650 / 0.3440\n",
            "[14/20][300/469]\tLoss_D: 0.5620\tLoss_G: 2.5940\tD(x): 0.7788\tD(G(z)): 0.2249 / 0.1048\n",
            "[14/20][350/469]\tLoss_D: 0.4132\tLoss_G: 2.7783\tD(x): 0.8265\tD(G(z)): 0.1780 / 0.0886\n",
            "[14/20][400/469]\tLoss_D: 0.7321\tLoss_G: 1.2616\tD(x): 0.5584\tD(G(z)): 0.0647 / 0.3372\n",
            "[14/20][450/469]\tLoss_D: 0.3551\tLoss_G: 2.5094\tD(x): 0.8210\tD(G(z)): 0.1278 / 0.1045\n",
            "[15/20][0/469]\tLoss_D: 0.6404\tLoss_G: 2.0217\tD(x): 0.8122\tD(G(z)): 0.3043 / 0.1694\n",
            "[15/20][50/469]\tLoss_D: 0.6443\tLoss_G: 1.7035\tD(x): 0.6248\tD(G(z)): 0.0912 / 0.2361\n",
            "[15/20][100/469]\tLoss_D: 0.5995\tLoss_G: 2.7700\tD(x): 0.9236\tD(G(z)): 0.3657 / 0.0833\n",
            "[15/20][150/469]\tLoss_D: 0.4455\tLoss_G: 2.0245\tD(x): 0.8829\tD(G(z)): 0.2464 / 0.1712\n",
            "[15/20][200/469]\tLoss_D: 0.7231\tLoss_G: 1.6876\tD(x): 0.6801\tD(G(z)): 0.2329 / 0.2307\n",
            "[15/20][250/469]\tLoss_D: 0.6772\tLoss_G: 1.8984\tD(x): 0.7543\tD(G(z)): 0.2841 / 0.1881\n",
            "[15/20][300/469]\tLoss_D: 0.5177\tLoss_G: 2.2404\tD(x): 0.8569\tD(G(z)): 0.2700 / 0.1349\n",
            "[15/20][350/469]\tLoss_D: 0.3455\tLoss_G: 2.4344\tD(x): 0.8681\tD(G(z)): 0.1642 / 0.1173\n",
            "[15/20][400/469]\tLoss_D: 0.4751\tLoss_G: 1.9125\tD(x): 0.8937\tD(G(z)): 0.2732 / 0.1863\n",
            "[15/20][450/469]\tLoss_D: 0.6858\tLoss_G: 1.9398\tD(x): 0.6238\tD(G(z)): 0.1270 / 0.1868\n",
            "[16/20][0/469]\tLoss_D: 0.5829\tLoss_G: 2.8464\tD(x): 0.8933\tD(G(z)): 0.3357 / 0.0790\n",
            "[16/20][50/469]\tLoss_D: 0.5400\tLoss_G: 2.3402\tD(x): 0.9089\tD(G(z)): 0.3117 / 0.1392\n",
            "[16/20][100/469]\tLoss_D: 0.5010\tLoss_G: 1.9176\tD(x): 0.7634\tD(G(z)): 0.1756 / 0.1820\n",
            "[16/20][150/469]\tLoss_D: 0.4550\tLoss_G: 2.4707\tD(x): 0.8363\tD(G(z)): 0.2156 / 0.1076\n",
            "[16/20][200/469]\tLoss_D: 0.7742\tLoss_G: 1.6869\tD(x): 0.6007\tD(G(z)): 0.1594 / 0.2423\n",
            "[16/20][250/469]\tLoss_D: 0.5063\tLoss_G: 1.9043\tD(x): 0.8213\tD(G(z)): 0.2306 / 0.1953\n",
            "[16/20][300/469]\tLoss_D: 0.6049\tLoss_G: 3.2980\tD(x): 0.8988\tD(G(z)): 0.3484 / 0.0546\n",
            "[16/20][350/469]\tLoss_D: 0.4360\tLoss_G: 2.2157\tD(x): 0.7527\tD(G(z)): 0.1098 / 0.1511\n",
            "[16/20][400/469]\tLoss_D: 0.5443\tLoss_G: 3.2425\tD(x): 0.8687\tD(G(z)): 0.2926 / 0.0546\n",
            "[16/20][450/469]\tLoss_D: 0.4907\tLoss_G: 1.2713\tD(x): 0.7235\tD(G(z)): 0.1195 / 0.3366\n",
            "[17/20][0/469]\tLoss_D: 0.4100\tLoss_G: 2.8565\tD(x): 0.8816\tD(G(z)): 0.2235 / 0.0764\n",
            "[17/20][50/469]\tLoss_D: 0.4885\tLoss_G: 1.5251\tD(x): 0.7004\tD(G(z)): 0.0800 / 0.2668\n",
            "[17/20][100/469]\tLoss_D: 0.6295\tLoss_G: 1.4352\tD(x): 0.6335\tD(G(z)): 0.1075 / 0.2946\n",
            "[17/20][150/469]\tLoss_D: 0.4713\tLoss_G: 2.3631\tD(x): 0.8393\tD(G(z)): 0.2179 / 0.1301\n",
            "[17/20][200/469]\tLoss_D: 0.5037\tLoss_G: 2.7135\tD(x): 0.8515\tD(G(z)): 0.2562 / 0.0938\n",
            "[17/20][250/469]\tLoss_D: 0.4554\tLoss_G: 1.4721\tD(x): 0.7167\tD(G(z)): 0.0832 / 0.2705\n",
            "[17/20][300/469]\tLoss_D: 0.9383\tLoss_G: 2.6767\tD(x): 0.9045\tD(G(z)): 0.5093 / 0.0995\n",
            "[17/20][350/469]\tLoss_D: 0.4722\tLoss_G: 2.6557\tD(x): 0.7858\tD(G(z)): 0.1728 / 0.0975\n",
            "[17/20][400/469]\tLoss_D: 0.6121\tLoss_G: 2.2799\tD(x): 0.7542\tD(G(z)): 0.2389 / 0.1258\n",
            "[17/20][450/469]\tLoss_D: 0.6064\tLoss_G: 3.0107\tD(x): 0.9231\tD(G(z)): 0.3615 / 0.0693\n",
            "[18/20][0/469]\tLoss_D: 0.4309\tLoss_G: 2.1531\tD(x): 0.8526\tD(G(z)): 0.2117 / 0.1516\n",
            "[18/20][50/469]\tLoss_D: 0.4298\tLoss_G: 1.7564\tD(x): 0.7784\tD(G(z)): 0.1386 / 0.2197\n",
            "[18/20][100/469]\tLoss_D: 1.4426\tLoss_G: 4.4108\tD(x): 0.9725\tD(G(z)): 0.6866 / 0.0218\n",
            "[18/20][150/469]\tLoss_D: 0.6849\tLoss_G: 1.3230\tD(x): 0.5828\tD(G(z)): 0.0641 / 0.3324\n",
            "[18/20][200/469]\tLoss_D: 0.3990\tLoss_G: 2.9320\tD(x): 0.8705\tD(G(z)): 0.2061 / 0.0724\n",
            "[18/20][250/469]\tLoss_D: 0.6803\tLoss_G: 0.7333\tD(x): 0.5899\tD(G(z)): 0.0826 / 0.5278\n",
            "[18/20][300/469]\tLoss_D: 0.4336\tLoss_G: 2.5879\tD(x): 0.8154\tD(G(z)): 0.1667 / 0.1048\n",
            "[18/20][350/469]\tLoss_D: 0.4530\tLoss_G: 1.4658\tD(x): 0.7380\tD(G(z)): 0.1009 / 0.2896\n",
            "[18/20][400/469]\tLoss_D: 0.5289\tLoss_G: 1.8751\tD(x): 0.7585\tD(G(z)): 0.1838 / 0.1946\n",
            "[18/20][450/469]\tLoss_D: 0.3937\tLoss_G: 2.3618\tD(x): 0.7925\tD(G(z)): 0.1215 / 0.1378\n",
            "[19/20][0/469]\tLoss_D: 0.8330\tLoss_G: 1.9132\tD(x): 0.5700\tD(G(z)): 0.1514 / 0.2109\n",
            "[19/20][50/469]\tLoss_D: 0.4804\tLoss_G: 1.7844\tD(x): 0.7661\tD(G(z)): 0.1526 / 0.2071\n",
            "[19/20][100/469]\tLoss_D: 0.7977\tLoss_G: 1.1833\tD(x): 0.5135\tD(G(z)): 0.0316 / 0.3640\n",
            "[19/20][150/469]\tLoss_D: 0.5051\tLoss_G: 2.1869\tD(x): 0.7411\tD(G(z)): 0.1375 / 0.1504\n",
            "[19/20][200/469]\tLoss_D: 0.7085\tLoss_G: 2.9056\tD(x): 0.8684\tD(G(z)): 0.3813 / 0.0741\n",
            "[19/20][250/469]\tLoss_D: 0.3897\tLoss_G: 2.7874\tD(x): 0.9076\tD(G(z)): 0.2323 / 0.0786\n",
            "[19/20][300/469]\tLoss_D: 0.4095\tLoss_G: 2.5353\tD(x): 0.8353\tD(G(z)): 0.1811 / 0.1108\n",
            "[19/20][350/469]\tLoss_D: 0.6248\tLoss_G: 1.5170\tD(x): 0.7283\tD(G(z)): 0.2154 / 0.2570\n",
            "[19/20][400/469]\tLoss_D: 0.5316\tLoss_G: 2.1161\tD(x): 0.8375\tD(G(z)): 0.2602 / 0.1587\n",
            "[19/20][450/469]\tLoss_D: 0.6673\tLoss_G: 4.0018\tD(x): 0.9190\tD(G(z)): 0.3866 / 0.0268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "fake = netG(noise)\n",
        "visTensor(fake[0:5].cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "XIjn0WKfsWBb",
        "outputId": "e6f2cdbc-356f-42a4-a592-87288599cd8c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
